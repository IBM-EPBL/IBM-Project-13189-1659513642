{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ac68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.models import load_model\n",
    "from skimage.transform import resize, rotate\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    validation_split=0.2,\n",
    "    )\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123d5588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6872 images belonging to 9 classes.\n",
      "Found 1714 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(\n",
    "    'Dataset4/training_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size=2862,\n",
    "    save_to_dir='Dataset4/aug_set',\n",
    "    class_mode='categorical',\n",
    "    color_mode=\"grayscale\",\n",
    "    subset=\"training\",\n",
    "    )\n",
    "x_test = train_datagen.flow_from_directory(\n",
    "    'Dataset4/training_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    color_mode=\"grayscale\",\n",
    "    subset=\"validation\",\n",
    "    )\n",
    "# x_test = train_datagen.flow_from_directory('Dataset/test_set', target_size=(64,64), batch_size=200, class_mode='categorical', color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211e6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d71d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32,(3,3),input_shape=(64,64,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(units=9,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226e851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f054d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 174s 48s/step - loss: 2.5709 - accuracy: 0.1345 - val_loss: 1.7873 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 160s 38s/step - loss: 2.3539 - accuracy: 0.1409 - val_loss: 1.9821 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 166s 40s/step - loss: 2.1897 - accuracy: 0.1307 - val_loss: 2.3236 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 164s 38s/step - loss: 2.1707 - accuracy: 0.1492 - val_loss: 1.9058 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 168s 42s/step - loss: 2.1520 - accuracy: 0.1895 - val_loss: 2.1733 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# model.fit(x_train, steps_per_epoch=24, epochs=10, validation_data=x_test, validation_steps=40)\n",
    "track = model.fit(\n",
    "    x_train,\n",
    "    steps_per_epoch=3,\n",
    "    epochs=5,\n",
    "    validation_data=x_test,\n",
    "    validation_steps=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892ff12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weight5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f65e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('weight3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "088c95af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.3288 - val_accuracy: 0.9728\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 81s 2s/step - loss: 0.0051 - accuracy: 0.9993 - val_loss: 0.2563 - val_accuracy: 0.9717\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 80s 2s/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.2908 - val_accuracy: 0.9722\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.3248 - val_accuracy: 0.9733\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.2781 - val_accuracy: 0.9706\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 102s 2s/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2732 - val_accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 118s 3s/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.3111 - val_accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 78s 2s/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.3731 - val_accuracy: 0.9606\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 70s 2s/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.3374 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.4158 - val_accuracy: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28183830460>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, steps_per_epoch=45, epochs=10, validation_data=x_test, validation_steps=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f390c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(frame):\n",
    "    img = resize(frame,(64,64,1))\n",
    "    # cv2.imshow(\"frame2\",img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    if np.max(img) > 1:\n",
    "        img = img/255\n",
    "    print(img.shape)\n",
    "    prediction = model.predict(img)\n",
    "    print(prediction)\n",
    "    prediction = np.where(prediction[:] > 0.5)[1] + 1\n",
    "    print(prediction[0])\n",
    "    # # frame=rotate(frame,45)\n",
    "    # # cv2.imshow(\"frame\",frame)\n",
    "    # # print(frame.shape)\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # thresh, frame = cv2.threshold(frame, 126, 255, cv2.THRESH_BINARY)\n",
    "    # print(frame.shape)\n",
    "    # kernel = np.ones((5,5),np.uint8)\n",
    "    # frame = cv2.morphologyEx(frame, cv2.MORPH_CLOSE, kernel)\n",
    "    # frame = cv2.morphologyEx(frame, cv2.MORPH_OPEN, kernel)\n",
    "    # # contours, _ = cv2.findContours(frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    # # contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    # # print(contours[1:])\n",
    "    # # cv2.drawContours(frame, contours, -1, (0,255,0), 3)\n",
    "    # cv2.imshow(\"frame1\",frame)\n",
    "    # cv2.imshow(\"frame2\",img)\n",
    "    # cv2.waitKey(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934ccbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 1)\n",
      "1/1 [==============================] - 1s 554ms/step\n",
      "[[1.4358401e-10 1.2662126e-06 9.9398309e-01 2.2878355e-07 2.4062178e-06\n",
      "  3.8932950e-09 6.0052965e-03 7.7619898e-06 1.3038267e-09]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "frame = cv2.imread(r\"Dataset\\\\test_set\\\\C\\\\150.png\")\n",
    "# frame = cv2.imread(r\"Dataset\\\\test_set\\B\\\\197.png\")\n",
    "data = detect(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1f6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72944136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 64, 64, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b2ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "94b06c7f1234a43955a0daeb3b41026d5f67c348d7d93406f6a02ba3227d6863"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
